{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDELT Knowledge Graph data pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign fields name\n",
    "fields = ['GKGRECORDID','DATE','SOURCECOLLECTIONIDENTIFIER','SOURCECOMMONNAME','DOCUMENTIDENTIFIER',\n",
    "          'V1COUNTS','V2COUNTS','THEMES','V2THEMES','LOCATIONS','V2LOCATIONS',\n",
    "          'PERSONS','V2PERSONS','ORGANIZATIONS','V2ORGANIZATIONS','TONE','V2DATES',\n",
    "          'GCAM','SHARINGIMAGE','RELATEDIMAGES','SOCIALIMAGEEMBEDS','SOCIALVIDEOEMBEDS',\n",
    "          'QUOTATIONS','ALLNAMES','AMOUNTS','TRANSLATIONINFO','EXTRASXML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_us_locations(full_loc_str):\n",
    "    loc_str_list = full_loc_str.split(';')\n",
    "    loc_us_list = []\n",
    "    loc_us_dict = {}\n",
    "    for loc_str in loc_str_list:\n",
    "        if int(loc_str.split('#')[0]) == 2 or int(loc_str.split('#')[0]) == 3:\n",
    "            #loc_us_list.append((loc_str.split('#')[0],loc_str.split('#')[1],loc_str.split('#')[5],loc_str.split('#')[6]))\n",
    "            loc_us_dict[loc_str.split('#')[1]] = (loc_str.split('#')[0],loc_str.split('#')[1],loc_str.split('#')[5],loc_str.split('#')[6])\n",
    "    return loc_us_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_theme_by_key(full_theme_str,key):\n",
    "    theme_str_list = full_theme_str.split(';')\n",
    "    flag = False\n",
    "    match_themes = []\n",
    "    for theme_str in theme_str_list:\n",
    "        res = re.findall(f'(.*?{key}.*?),',theme_str)\n",
    "        if len(res) > 0:\n",
    "            match_themes.append(res[0])\n",
    "            flag=True\n",
    "    return flag,match_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test performance for one file\n",
    "gkg_pull_one = pd.read_csv('20201217153000.gkg.csv',sep='\\t',engine='python',header=None)\n",
    "gkg_pull_one.columns = fields\n",
    "results = []\n",
    "for record_tuple in gkg_pull_one[gkg_pull_one.SOURCECOLLECTIONIDENTIFIER==1].itertuples():\n",
    "    full_theme_str = record_tuple[9]\n",
    "    if not full_theme_str or full_theme_str is np.nan:\n",
    "        #no theme\n",
    "        pass\n",
    "    else:\n",
    "        #print(full_theme_str)\n",
    "        flag,match_themes = extract_theme_by_key(full_theme_str,'CLIMATE.?CHANGE')\n",
    "        #flag2,match_themes2 = extract_theme_by_key(full_theme_str,'CLIMATECHANGE')\n",
    "        \n",
    "        if flag:\n",
    "            #print(match_themes)\n",
    "            full_loc_str = record_tuple[11]\n",
    "            #print(full_loc_str)\n",
    "            if not full_loc_str or full_loc_str is np.nan:\n",
    "                pass\n",
    "            else:\n",
    "                us_locations = extract_us_locations(full_loc_str)\n",
    "                if (len(us_locations) > 0):\n",
    "                    results.append((record_tuple[1],record_tuple[5],us_locations))\n",
    "\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all gkg file paths and download latest 500\n",
    "gkg_file_list = []\n",
    "with open('masterfilelist.txt') as fo:\n",
    "    lines = fo.readlines()\n",
    "for line in lines[-500:]:\n",
    "    url = line.split(' ')[-1].rstrip('\\n')\n",
    "    res = re.match('.*.gkg..*',url)\n",
    "    if res:\n",
    "        #print(url)\n",
    "        gkg_file_list.append(url)\n",
    "\n",
    "# extract geoinformation for climate related news\n",
    "results = []\n",
    "error_fpaths = []\n",
    "t0 = time.time()\n",
    "for fpath in gkg_file_list:\n",
    "    try:\n",
    "        gkg_pull_one = pd.read_csv(fpath,sep='\\t',engine='python',header=None,encoding='latin_1')\n",
    "    except:\n",
    "        print(f'error met {fpath}')\n",
    "        error_fpaths.append(fpath)\n",
    "        continue\n",
    "    gkg_pull_one.columns = fields\n",
    "    #test performance for one file\n",
    "    for record_tuple in gkg_pull_one[gkg_pull_one.SOURCECOLLECTIONIDENTIFIER == 1].itertuples():\n",
    "        full_theme_str = record_tuple[9]\n",
    "        #print(full_theme_str)\n",
    "        if not full_theme_str or full_theme_str is np.nan:\n",
    "            #no theme\n",
    "            pass\n",
    "        else:\n",
    "            #print(len(full_theme_str))\n",
    "            flag,match_themes = extract_theme_by_key(full_theme_str,'CLIMATE.?CHANGE')\n",
    "            if flag:\n",
    "                full_loc_str = record_tuple[11]\n",
    "                if not full_loc_str or full_loc_str is np.nan:\n",
    "                    pass\n",
    "                else:\n",
    "                    us_locations = extract_us_locations(full_loc_str)\n",
    "                    if (len(us_locations) > 0):\n",
    "                        results.append((record_tuple[1],record_tuple[5],us_locations))\n",
    "    t1 = time.time()\n",
    "    print(f'{fpath} accumulated run time {t1-t0}s')\n",
    "len(results)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Google BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import pandas as pd\n",
    "client = bigquery.Client()\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "\n",
    "# Perform a query.\n",
    "QUERY = (\n",
    "\n",
    "'''\n",
    "    WITH subset_query AS (\n",
    "  SELECT *  from gdelt-bq.gdeltv2.gkg \n",
    "  where DATE>20170302000000 and DATE < 20170304000000 \n",
    " limit 200\n",
    ")\n",
    "    select gid,regex_extr, ANY_VALUE(code) as level_code,ANY_VALUE(url) as url,ANY_VALUE(location) as raw_loc from \n",
    "    (select url,gid,location,regexp_extract(location, r'^[2-3]#(.*?)#') as regex_extr,\n",
    "    regexp_extract(location, r'^([2-3])#.*?') as code \n",
    "    from\n",
    "    (select subset_query.GKGRECORDID as gid ,subset_query.DocumentIdentifier as url, SPLIT(subset_query.V2Locations, ';') as split_loc\n",
    "    \n",
    "    from subset_query) CROSS JOIN UNNEST(split_loc) as location)\n",
    "    where regex_extr IS NOT NULL\n",
    "    group by gid,regex_extr\n",
    "'''\n",
    "\n",
    ")\n",
    "#print(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = (\n",
    "    client.query(QUERY)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")\n",
    "#print(dataframe.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
